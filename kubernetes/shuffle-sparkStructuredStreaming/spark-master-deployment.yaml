kind: Deployment
apiVersion: apps/v1
metadata:
  name: spark-master
spec:
  replicas: 1
  selector:
    matchLabels:
      component: spark-master
  template:
    metadata:
      labels:
        component: spark-master
    spec:
      containers:
        - name: spark-master
          #image: ghcr.io/dynatrace-research/shufflebench/shuffle-spark:latest -> THIS IMAGE DOES NOT WORK FOR FAULT TOLERANCE IN SPARK. IT IS NECESSARY TO BUILD A NEW IMAGE WITH THE CHANGES IN SPARK'S CODE (SEE SHUFFLE-SPARK MODULE).
          imagePullPolicy: Always
          command: ["/spark-master"]
          ports:
            - containerPort: 7077
            - containerPort: 8080
          resources:
            limits:
              memory: 2Gi
              cpu: 300m
        - name: spark-submit
          #image: ghcr.io/dynatrace-research/shufflebench/shuffle-spark:latest -> THIS IMAGE DOES NOT WORK FOR FAULT TOLERANCE IN SPARK. IT IS NECESSARY TO BUILD A NEW IMAGE WITH THE CHANGES IN SPARK'S CODE (SEE SHUFFLE-SPARK MODULE).
          env:
            - name: KAFKA_BOOTSTRAP_SERVERS
              value: "theodolite-kafka-kafka-bootstrap:9092"
            - name: MATCHER_ZIPF_NUM_RULES
              value: "1000000"
            - name: MATCHER_ZIPF_TOTAL_SELECTIVITY
              value: "0.2"
            - name: MATCHER_ZIPF_S
              value: "0.0"
            - name: CONSUMER_INIT_COUNT_RANDOM
              value: "true"
            - name: KAFKA_TOPIC_INPUT
              value: "input"
            - name: KAFKA_TOPIC_OUTPUT
              value: "output"
            - name: SPARK_EXECUTOR_MEMORY
              value: "2G"
            - name: SPARK_SQL_SHUFFLE_PARTITIONS
              value: "40"
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          command:
            - bash
            - -c
            - >
              /opt/spark/bin/spark-submit \
                --master spark://spark-master:7077 \
                --deploy-mode client \
                --class com.dynatrace.research.shufflebench.SparkStructuredStreamingShuffle \
                --conf spark.driver.host=$POD_IP \
                --conf spark.driver.bindAddress=$POD_IP \
                --conf spark.executor.memory=$SPARK_EXECUTOR_MEMORY \
                --conf spark.sql.shuffle.partitions=$SPARK_SQL_SHUFFLE_PARTITIONS \
                /shuffle-spark-1.0-SNAPSHOT.jar
          ports:
            - containerPort: 4040
          resources:
            limits:
              memory: 2Gi
              cpu: 1000m #400m
#          volumeMounts:
#            - name: shared-data
#              mountPath: /tmp/spark/ # to store the checkpoints
          volumeMounts:
            - name: persistent-storage
              mountPath: /tmp/spark/ # to store the checkpoints. EFS was used to provide the storage infrastructure following all the steps from <https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html>
        - name: offset-committer
          image: ghcr.io/dynatrace-research/shufflebench/shuffle-spark-offset-committer:latest
          env:
            - name: KAFKA_BOOTSTRAP_SERVERS
              value: "theodolite-kafka-kafka-bootstrap:9092"
            - name: COMMIT_INTERVAL_MS
              value: "300"
          resources:
            limits:
              memory: 2Gi
              cpu: 300m
#          volumeMounts:
#            - name: shared-data
#              mountPath: /tmp/spark/ # to store the checkpoints
          volumeMounts:
            - name: persistent-storage
              mountPath: /tmp/spark/ # to store the checkpoints
      initContainers: #init container to delete older checkpoints
        - name: set-folder-ownership
          image: busybox:1.33.1  # You can use a minimal image like BusyBox for this task
          command: ["/bin/sh", "-c"]
          args: ["rm -rf /tmp/spark/*"]# && chown -R spark /tmp/spark"]
          securityContext:
            runAsUser: 0  # Specify the user ID you want to use (0 for root)
          volumeMounts:
          - name: persistent-storage
            mountPath: /tmp/spark  # Mount path inside the init container

      nodeSelector:
        type: manager
      volumes:
#        - name: shared-data
#          emptyDir: {}
        - name: persistent-storage
          persistentVolumeClaim:
            claimName: efs-claim #change accordingly to your volume and filesystem. EFS was used to provide the storage infrastructure following all the steps from <https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html>